{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = \"esquistossomose.parquet\"\n",
    "codigos_estados = {35: 'SP', 41: 'PR', 42: 'SC', 43: 'RS', 50: 'MS', 11: 'RO', 12: 'AC', 13: 'AM', 14: 'RR', 15: 'PA', 16: 'AP', 17: 'TO', 21: 'MA', 24: 'RN', 25: 'PB', 26: 'PE', 27: 'AL', 28: 'SE', 29: 'BA', 31: 'MG', 33: 'RJ', 51: 'MT', 52: 'GO', 53: 'DF', 22: 'PI', 23: 'CE', 32: 'ES'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questão 1\n",
    "\n",
    "Para essa questão, foi notado que o comprimento do index seria igual o número de casos da questão.    \n",
    "Assim, basta ler o arquivo e calcular o comprimento do index da variável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4219"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def questao_1(datapath = DATA):\n",
    "    tabela = pd.read_parquet(datapath)\n",
    "    # Procura o comprimento do index, que é a quantidade de casos notificados\n",
    "    tamanho = len(tabela.index)\n",
    "    return tamanho\n",
    "questao_1()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questão 3\n",
    "Para essa questão, foi utilizada a função `value_counts` presente na biblioteca do Pandas, a qual serve para agrupar todos os dados presentes numa tabela, contando a quantidade de valores desses dados.  \n",
    "\n",
    "A função cria um objeto que é uma \"series\", o qual pode facilmente ser convertido em um dicionário, com a função `dict`.  \n",
    "Após isso, foi feita uma lista com as chaves do dicionário que foi criado.  \n",
    "\n",
    "Como a função `value_counts` sempre ordena os valores do maior para o menor, sabemos que quando o primeiro elemento for pego, teremos o sexo com maior quantidade de aparições.  \n",
    "\n",
    "Por fim, basta retornar tanto a string com o sexo que teve a maior quantidade de aparições quanto o dicionário com os sexos e suas quantidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('M', {'M': 2558, 'F': 1661})"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def questao_3(datapath = DATA):\n",
    "    tabela = pd.read_parquet(datapath)\n",
    "    # Criando um dicionário com os sexos e a quantidade de ocorrências de homem e mulher\n",
    "    casos_por_sexo = dict(tabela[\"CS_SEXO\"].value_counts())\n",
    "    # Criando lista com as quantidades de casos\n",
    "    lista_com_casos = list(casos_por_sexo.keys())\n",
    "    # Pegando o maior elemento (primeiro elemento)\n",
    "    maior_quantidade = lista_com_casos[0]\n",
    "    # Retornando o sexo que aparece em maior quantidade, bem como a frequência\n",
    "    return maior_quantidade, casos_por_sexo\n",
    "questao_3()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questão 7\n",
    "Para essa questão, além da variável \"codigos_estados\" já definida, foi definido um dicionário no qual as chaves são os códigos dos estados e os valores, a quantidade de municípios dos estados.  \n",
    "\n",
    "Utilizamos um `groupby` nos estados, contando a quantidade de municípios que apareciam. Para evitar a repetição de termos, utilizamos a função `nunique`, a qual retira os municípios que seriam contados duas vezes.  \n",
    "\n",
    "Após isso, criaremos um dataframe auxiliar no qual tranformaremos o dicionário que temos num dataframe e adicionaremos uma nova coluna a ele com a quantidade de casos por estado.  \n",
    "\n",
    "Agora, temos um dataframe com a quantidade de cidades de cada UF, bem como a quantidade de cidades que apresentaram a doença.  Portanto, podemos criar uma nova coluna, como sendo a divisão de uma pela outra, para termos a proporção e multiplicaremos por 100 para a obtenção da porcentagem.   \n",
    "\n",
    "Para obtermos as siglas das UFs, utilizaremos a função `map`, para assim auxiliar na conversão dos códigos nas siglas das unidades da federação.  \n",
    "\n",
    "Por fim, vamos criar um dicionário no qual vamos zipar a coluna UF com seu respectivo valor de porcentagem e retorná-lo.  \n",
    "\n",
    "Assim temos uma função que nos retorna cada UF com a respectiva proporção de municípios que relataram casos em cada estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MG': 17.82,\n",
       " 'SP': 16.43,\n",
       " 'RS': 0.4,\n",
       " 'BA': 22.54,\n",
       " 'PR': 4.76,\n",
       " 'SC': 3.39,\n",
       " 'GO': 3.66,\n",
       " 'PI': 0.45,\n",
       " 'PB': 5.83,\n",
       " 'MA': 3.69,\n",
       " 'PE': 17.39,\n",
       " 'CE': 5.98,\n",
       " 'RN': 2.4,\n",
       " 'PA': 3.47,\n",
       " 'MT': 3.55,\n",
       " 'TO': 2.16,\n",
       " 'AL': 30.39,\n",
       " 'RJ': 16.3,\n",
       " 'MS': 6.33,\n",
       " 'ES': 42.31,\n",
       " 'SE': 32.0,\n",
       " 'AM': 0.0,\n",
       " 'RO': 21.15,\n",
       " 'AC': 0.0,\n",
       " 'AP': 12.5,\n",
       " 'RR': 13.33}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def questao_7(datapath = DATA):\n",
    "    tabela = pd.read_parquet(datapath)\n",
    "    # Dicionário com o código do estado e a quantidade de municípios nele\n",
    "    mun_por_est = {31 : 853, 35 : 645, 43 : 497, 29 : 417, 41 : 399, 42 : 295, 52 : 246, 22 : 224, 25 : 223, 21: 217, 26 : 184, 23 : 184, 24 : 167, 15 : 144, 51 : 141, 17 : 139, 27 : 102, 33 : 92, 50 : 79, 32 : 78, 28 : 75, 13 : 62,11 : 52, 12 : 22, 16 : 16, 14 : 15}\n",
    "    # Agrupando os dados por estado e contando cada município uma única vez\n",
    "    casos_por_estado = tabela.groupby('SG_UF_NOT')['ID_MUNICIP'].nunique()\n",
    "    # Convertendo o index para um número para não dar problema no dicionário\n",
    "    casos_por_estado.index = pd.to_numeric(casos_por_estado.index)\n",
    "    # Criando dataframe auxiliar\n",
    "    tabela_auxiliar = pd.DataFrame({\"COD\": mun_por_est.keys(), \"Quantidade de Cidades\": mun_por_est.values()})\n",
    "    # Colocando a quantidade de cidades com casos como uma coluna na tabela\n",
    "    tabela_auxiliar[\"Quantidade de cidades com casos\"] = tabela_auxiliar[\"COD\"].map(casos_por_estado).fillna(0)\n",
    "    # Criando a coluna \"Porcentagem\"\n",
    "    tabela_auxiliar[\"Porcentagem\"] = (tabela_auxiliar[\"Quantidade de cidades com casos\"]*100/tabela_auxiliar[\"Quantidade de Cidades\"]).round(2)\n",
    "    # Criando uma coluna com as siglas de cada UF\n",
    "    tabela_auxiliar[\"UF\"] = tabela_auxiliar[\"COD\"].map(codigos_estados)\n",
    "    # Criando um dicionário com as siglas e as porcentagens\n",
    "    dicionario = dict(zip(tabela_auxiliar[\"UF\"], tabela_auxiliar[\"Porcentagem\"]))\n",
    "    # Retornando o dicionário\n",
    "    return dicionario\n",
    "questao_7()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questão 9\n",
    "Nessa questão utilizaremos as colunas já previamente realizadas na questão 8. Assim, apenas copiaremos o código.  \n",
    "\n",
    "No entanto, como o formato presente na coluna não é favorável à manipulação dos dados, transformaremos as datas do tipo \"deltatime\" para inteiros com o método `dt.days`.  \n",
    "\n",
    "Após isso, foi criado um dataframe com os dados agrupados por UF, no qual temos as colunas com a média e o desvio padrão dos atrasos.  \n",
    "\n",
    "Também foi adicionada uma coluna que transforma os códigos presentes no texto nas siglas das UFs.  \n",
    "\n",
    "Por fim, foi feito um dicionário, \"zipando\" a coluna UF com um zip entre a média e o desvio padrão.  \n",
    "\n",
    "Assim, temos um dicionário no qual as chaves são as siglas de UF e os valores são listas, retornando as médias e os desvios padrões em dias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RO': (365.98, 2277.71),\n",
       " 'RR': (7.0, 7.81),\n",
       " 'PA': (11.75, 18.44),\n",
       " 'AP': (191.0, 230.52),\n",
       " 'TO': (72.25, 101.12),\n",
       " 'MA': (65.04, 63.38),\n",
       " 'PI': (40.0, 0.0),\n",
       " 'CE': (41.57, 64.7),\n",
       " 'RN': (49.73, 60.82),\n",
       " 'PB': (135.11, 294.21),\n",
       " 'PE': (159.64, 498.15),\n",
       " 'AL': (951.63, 3669.89),\n",
       " 'SE': (62.6, 323.87),\n",
       " 'BA': (163.04, 1228.63),\n",
       " 'MG': (113.22, 829.79),\n",
       " 'ES': (93.91, 560.26),\n",
       " 'RJ': (183.59, 233.38),\n",
       " 'SP': (405.78, 2205.21),\n",
       " 'PR': (273.92, 1232.03),\n",
       " 'SC': (22.67, 67.27),\n",
       " 'RS': (11.0, 12.73),\n",
       " 'MS': (318.78, 597.05),\n",
       " 'MT': (615.45, 1085.11),\n",
       " 'GO': (65.5, 70.96),\n",
       " 'DF': (254.0, 454.5)}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def questao_9(datapath = DATA):\n",
    "    tabela = pd.read_parquet(datapath)\n",
    "    # Criando as colunas com as datas\n",
    "    tabela[\"DT_NOTIFICACAO\"] = pd.to_datetime(tabela[\"DT_NOTIFIC\"])\n",
    "    tabela[\"DT_SINTOMAS\"] = pd.to_datetime(tabela[\"DT_SIN_PRI\"])\n",
    "    tabela[\"ATRASO_NOT\"] = tabela[\"DT_NOTIFICACAO\"] - tabela[\"DT_SINTOMAS\"]\n",
    "    # Transformando os números da tabela de datas para inteiros\n",
    "    tabela[\"ATRASO_NOT\"] = tabela[\"ATRASO_NOT\"].dt.days\n",
    "    # Criando o dataframe com a média e o desvio padrão\n",
    "    media_atraso = tabela.groupby(\"SG_UF_NOT\")[\"ATRASO_NOT\"].agg([\"mean\", \"std\"]).fillna(0)\n",
    "    # Colocando o index como número para funcionar\n",
    "    media_atraso.index = pd.to_numeric(media_atraso.index)\n",
    "    # Criando uma coluna com as siglas de cada UF\n",
    "    media_atraso[\"UF\"] = media_atraso.index.map(codigos_estados)\n",
    "    # Criando o dicionário\n",
    "    resposta = dict(zip(media_atraso[\"UF\"], zip(media_atraso[\"mean\"].round(2), media_atraso[\"std\"].round(2))))\n",
    "    return resposta\n",
    "questao_9()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
